"""
Script to analyze and list all question patterns generated by generate_dataset_large.py

Usage:
    python chatbot/Q_and_A/analyze_patterns.py --output_dir chatbot/Q_and_A/output_large_test
"""

import argparse
import json
import pandas as pd
from collections import defaultdict, Counter
from typing import Dict, List, Tuple


def parse_reasoning_path(path_str: str) -> List[str]:
    """Parse reasoning path from JSON string."""
    try:
        return json.loads(path_str)
    except:
        return path_str.split(' -> ')


def extract_pattern_signature(path: List[str]) -> Tuple[str, List[str]]:
    """
    Extract pattern signature from reasoning path.
    Returns (signature, edge_types)
    
    Example:
    - ["pol123", "BORN_AT", "loc456"] -> ("Politician->BORN_AT->Location", ["BORN_AT"])
    - ["pol1", "SUCCEEDED", "pol2", "SERVED_AS", "pos3"] -> ("Politician->SUCCEEDED->Politician->SERVED_AS->Position", ["SUCCEEDED", "SERVED_AS"])
    """
    edge_types = []
    node_types = []
    
    for i, item in enumerate(path):
        if i % 2 == 0:  # Node
            # Extract node type from ID
            if item.startswith('pol'):
                node_types.append('Politician')
            elif item.startswith('pos'):
                node_types.append('Position')
            elif item.startswith('loc'):
                node_types.append('Location')
            elif item.startswith('award'):
                node_types.append('Award')
            elif item.startswith('mil'):
                node_types.append('Military')
            elif item.startswith('cam'):
                node_types.append('Campaign')
            elif item.startswith('alma'):
                node_types.append('School')
            elif item.startswith('title'):
                node_types.append('Title')
            elif item.startswith('YEAR_'):
                node_types.append('Year')
            elif item.startswith('DURATION_'):
                node_types.append('Duration')
            else:
                node_types.append('Unknown')
        else:  # Edge
            edge_types.append(item)
    
    # Build signature
    signature_parts = []
    for i, node_type in enumerate(node_types):
        signature_parts.append(node_type)
        if i < len(edge_types):
            signature_parts.append(edge_types[i])
    
    signature = '->'.join(signature_parts)
    
    return signature, edge_types


def analyze_patterns(output_dir: str):
    """Analyze and display all patterns from generated dataset."""
    
    print("=" * 80)
    print("PATTERN ANALYSIS FOR GENERATED DATASET")
    print("=" * 80)
    
    # Load questions
    mcq_questions = pd.read_csv(f"{output_dir}/mcq_questions.csv", encoding='utf-8')
    tf_questions = pd.read_csv(f"{output_dir}/true_false_questions.csv", encoding='utf-8')
    
    total_questions = len(mcq_questions) + len(tf_questions)
    print(f"\nTotal questions: {total_questions}")
    print(f"  MCQ: {len(mcq_questions)}")
    print(f"  TRUE_FALSE: {len(tf_questions)}")
    
    # Combine all questions
    all_questions = []
    for _, row in mcq_questions.iterrows():
        all_questions.append({
            'type': 'MCQ',
            'hop_count': row['hop_count'],
            'reasoning_path': parse_reasoning_path(row['reasoning_path']),
            'variant_type': row['variant_type']
        })
    
    for _, row in tf_questions.iterrows():
        all_questions.append({
            'type': 'TRUE_FALSE',
            'hop_count': row['hop_count'],
            'reasoning_path': parse_reasoning_path(row['reasoning_path']),
            'variant_type': row['variant_type']
        })
    
    # Statistics by hop count
    print("\n" + "=" * 80)
    print("DISTRIBUTION BY HOP COUNT")
    print("=" * 80)
    
    hop_counts = Counter(q['hop_count'] for q in all_questions)
    for hop in sorted(hop_counts.keys()):
        percentage = (hop_counts[hop] / total_questions) * 100
        print(f"  {hop}-hop: {hop_counts[hop]:5d} questions ({percentage:5.2f}%)")
    
    # Statistics by variant type
    print("\n" + "=" * 80)
    print("DISTRIBUTION BY VARIANT TYPE")
    print("=" * 80)
    
    variant_counts = Counter(q['variant_type'] for q in all_questions)
    for variant, count in sorted(variant_counts.items()):
        percentage = (count / total_questions) * 100
        print(f"  {variant}: {count:5d} questions ({percentage:5.2f}%)")
    
    # Analyze patterns by hop count
    print("\n" + "=" * 80)
    print("UNIQUE PATTERNS BY HOP COUNT")
    print("=" * 80)
    
    patterns_by_hop = defaultdict(lambda: defaultdict(int))
    edge_combinations_by_hop = defaultdict(lambda: Counter())
    
    for q in all_questions:
        hop = q['hop_count']
        signature, edge_types = extract_pattern_signature(q['reasoning_path'])
        patterns_by_hop[hop][signature] += 1
        edge_combinations_by_hop[hop][tuple(edge_types)] += 1
    
    total_unique_patterns = 0
    
    for hop in sorted(patterns_by_hop.keys()):
        patterns = patterns_by_hop[hop]
        print(f"\n{hop}-HOP PATTERNS ({len(patterns)} unique patterns, {hop_counts[hop]} questions):")
        print("-" * 80)
        
        # Show top 10 most common patterns
        sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)
        for i, (pattern, count) in enumerate(sorted_patterns[:10], 1):
            print(f"  {i:2d}. {pattern}")
            print(f"      Count: {count} questions")
        
        if len(sorted_patterns) > 10:
            print(f"  ... and {len(sorted_patterns) - 10} more patterns")
        
        total_unique_patterns += len(patterns)
        
        # Show edge combinations
        print(f"\n  Edge type combinations ({hop}-hop):")
        edge_combos = edge_combinations_by_hop[hop]
        for i, (edges, count) in enumerate(edge_combos.most_common(10), 1):
            edge_str = ' -> '.join(edges)
            print(f"    {i:2d}. {edge_str}: {count} questions")
    
    print("\n" + "=" * 80)
    print(f"TOTAL UNIQUE PATTERNS ACROSS ALL HOPS: {total_unique_patterns}")
    print("=" * 80)
    
    # Detailed edge type usage
    print("\n" + "=" * 80)
    print("EDGE TYPE USAGE STATISTICS")
    print("=" * 80)
    
    all_edge_types = []
    for q in all_questions:
        _, edge_types = extract_pattern_signature(q['reasoning_path'])
        all_edge_types.extend(edge_types)
    
    edge_type_counts = Counter(all_edge_types)
    print(f"\nTotal edge uses: {len(all_edge_types)}")
    print(f"Unique edge types: {len(edge_type_counts)}")
    print("\nEdge type frequency:")
    for edge_type, count in edge_type_counts.most_common():
        percentage = (count / len(all_edge_types)) * 100
        print(f"  {edge_type:25s}: {count:5d} uses ({percentage:5.2f}%)")
    
    # Pattern examples by hop count
    print("\n" + "=" * 80)
    print("EXAMPLE PATTERNS FOR EACH HOP COUNT")
    print("=" * 80)
    
    for hop in sorted(patterns_by_hop.keys()):
        print(f"\n{hop}-HOP EXAMPLES:")
        print("-" * 80)
        
        # Get diverse examples (different patterns)
        examples = []
        seen_patterns = set()
        
        for q in all_questions:
            if q['hop_count'] == hop and len(examples) < 5:
                signature, _ = extract_pattern_signature(q['reasoning_path'])
                if signature not in seen_patterns:
                    examples.append(q)
                    seen_patterns.add(signature)
        
        for i, example in enumerate(examples, 1):
            signature, edge_types = extract_pattern_signature(example['reasoning_path'])
            edge_str = ' -> '.join(edge_types)
            print(f"  Example {i}: {signature}")
            print(f"            Edges: {edge_str}")
    
    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)
    print(f"Total questions generated: {total_questions}")
    print(f"Total unique patterns: {total_unique_patterns}")
    print(f"Hop count range: {min(hop_counts.keys())} to {max(hop_counts.keys())}")
    print(f"Edge types used: {len(edge_type_counts)}")
    print(f"Average questions per pattern: {total_questions / total_unique_patterns:.2f}")
    
    # Multi-hop vs single-hop ratio
    single_hop = hop_counts.get(1, 0)
    multi_hop = total_questions - single_hop
    print(f"\nSingle-hop (1-hop): {single_hop} ({(single_hop/total_questions)*100:.1f}%)")
    print(f"Multi-hop (2+ hops): {multi_hop} ({(multi_hop/total_questions)*100:.1f}%)")
    
    print("\n" + "=" * 80)


def main():
    parser = argparse.ArgumentParser(description="Analyze patterns in generated dataset")
    parser.add_argument('--output_dir', type=str, required=True,
                       help='Directory containing the generated dataset')
    
    args = parser.parse_args()
    analyze_patterns(args.output_dir)


if __name__ == '__main__':
    main()
